# -*- coding: utf-8 -*-
"""logstic regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15GGuhMWAjcUKc2s6KASJ_9FcsGXL5LGA
"""

import numpy as np
import matplotlib.pyplot as plt
import copy
from sklearn.linear_model import LogisticRegression

def f_x(x,w,b):
  f=np.dot(w,x)+b
  return f

def sig_moid(z):
  g=1/(1+np.exp(-z))

  return g

def loss_function(x,y,w,b,lambda_=1):
  m=x.shape[0]
  cost=0.0
  for i in range (m):
    z=f_x(x[i],w,b)
    f_wb_i=sig_moid(z)
    cost=cost+(y[i]*np.log(f_wb_i))+(1-y[i])*np.log(1-f_wb_i)
  cost=cost/(-m)
  reg_cost=0
  for j in range (len(w)):
    reg_cost=reg_cost+(w[j]**2)
  reg_cost=reg_cost*(lambda_/(2*m))
  cost=cost+reg_cost
  return cost

def gred(x,y,w,b,lambda_):
  m,n=x.shape
  dj_dw=np.zeros((n,))
  dj_db=0.0
  for i in range (m):
    z=f_x(x[i],w,b)
    f_wb_i=sig_moid(z)
    for j in range (n):
      dj_dw[j]=dj_dw[j]+(f_wb_i-y[i])*x[i][j]
    dj_db=dj_db+(f_wb_i-y[i])
  dj_dw=dj_dw/m
  dj_db=dj_db/m
  for j in range (n):
    dj_dw[j]=dj_dw[j]+(lambda_/m)*w[j]
  return dj_dw,dj_db

def  gradient_descent(x, y, w_in, b_in, alpha, num_iters,lambda_=1):
   w = copy.deepcopy(w_in)
   b = b_in

   for i in range (num_iters):
    dj_dw,dj_db=gred(x,y,w,b,lambda_)

    w=w-alpha*dj_dw
    b=b-alpha*dj_db
   return w,b

x =np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])

y = np.array([0, 0, 0, 1, 1, 1])

w_init=np.zeros(x.shape[1])
b_init=0.0
alpha = 0.1
num_iters = 1000
lambda_=1

w, b = gradient_descent(x, y, w_init, b_init, alpha, num_iters,lambda_)

print("Final weights:", w)
print("Final bias:", b)

# اختبار نموذجك على البيانات نفسها
def predict(x, w, b):
    z = f_x(x,w,b)
    return 1 if sig_moid(z)>0.5 else 0

predictions = [predict(xi, w, b) for xi in x]
print("Predictions:", predictions)
print("Actual labels:", y)

plt.scatter(x[y==0][:,0], x[y==0][:,1], color='blue', label='Class 0')
plt.scatter(x[y==1][:,0], x[y==1][:,1], color='red', label='Class 1')

# رسم decision boundary
x1_vals = np.linspace(0, 3.5, 100)
x2_vals = -(w[0]*x1_vals + b)/w[1]  # من w1*x1 + w2*x2 + b = 0
plt.plot(x1_vals, x2_vals, color='green', label='Decision Boundary')

plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Logistic Regression Decision Boundary')
plt.grid(True)
plt.show()